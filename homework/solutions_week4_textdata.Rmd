---
title: "Solutions: Text data & text mining"
subtite: "Homework week 4"
author: "B Kleinberg, D Hammocks, F Soldner"
subtitle: Advanced Crime Analysis, UCL
output: html_notebook
---

**SOLUTIONS**

### Task 1: Creating your Corpus

Select three assignments. Convert them to RAW text files. Import into R and create a corpus.

```{r}
#Load Packages
require(readr)
#Import Text files.


text1<-read_file('data/GutenbergBooks/AnAfricanMillionaire.txt')
text2<-read_file('data/GutenbergBooks/CrimeAndPunishment.txt')
text3<-read_file('data/GutenbergBooks/DeadMenTellNoTales.txt')

#Load Package
require(quanteda)

#Create a corpus
corpus<-corpus(c(text1, text2, text3))

```

### Task 2: Text Statistics

Calcualte the average number of characters per word and the average number of words per sentence.

```{r}
#Average number of characters per word
avgCharPerWord<-nchar(corpus[])/ntoken(corpus[])
#Display Answer#avgCharPerWord
avgCharPerWord


#Average number of words per sentence
avgWordPerSentence<-ntoken(corpus[])/nsentence(corpus[])
#Display Response
avgWordPerSentence
```

### Task 3: Text Metrics
What is the type-token ratio (TTR) of your texts (each text individually)?

```{r}
#TTR of each text
Text1_TTR<-ntype(corpus[1])/ntoken(corpus[1])
Text2_TTR<-ntype(corpus[2])/ntoken(corpus[2])
Text3_TTR<-ntype(corpus[3])/ntoken(corpus[3])

#Display The Answers
print(c(TTRText1, TTRText2, TTRText3))
```

### Task 4: Word Frequencies
Build a term frequency count representation, and retrieve the top features (hint: topfeatures) for each text.
```{r}
#Calculate Document Feature Matrices (Frequency Count Representation)
Text1_DFM<-dfm(corpus[1])
Text2_DFM<-dfm(corpus[2])
Text3_DFM<-dfm(corpus[3])

#USe topfeatures to get the top features
Text1_TF<-topfeatures(Text1_DFM)
Text2_TF<-topfeatures(Text2_DFM)
Text3_TF<-topfeatures(Text3_DFM)
```



### Task 5: TF-IDF
Now build a TF-IDF weighted representation of your corpus. Perform this transformation in five different ways: (1) based on the raw texts, (2) removing stopwords, (3) removing punctuation, (4) stemming the words, and (5) combining (2)-(4).

```{r}
#Calculate TF-IDF
Text1_TFIDFRaw<-dfm_tfidf(Text1_DFM)
Text2_TFIDFRaw<-dfm_tfidf(Text2_DFM)
Text3_TFIDFRaw<-dfm_tfidf(Text3_DFM)

#Caculate TF-IDF with stopwords removed
Text1_TFIDFSW<-dfm_tfidf(dfm(Text1_DFM, remove = stopwords("english")))
Text2_TFIDFSW<-dfm_tfidf(dfm(Text2_DFM, remove = stopwords("english")))
Text3_TFIDFSW<-dfm_tfidf(dfm(Text3_DFM, remove = stopwords("english")))

#Calcualte TF-IDF with punctuation removed
Text1_TFIDFPunc<-dfm_tfidf(dfm(corpus[1], remove_punct = TRUE))
Text2_TFIDFPunc<-dfm_tfidf(dfm(corpus[2], remove_punct = TRUE))
Text3_TFIDFPunc<-dfm_tfidf(dfm(corpus[3], remove_punct = TRUE))

#Calculate TF-IDF - words stemmed
Text1_TFIDFStem<-dfm_tfidf(dfm(Text1_DFM, stem=TRUE))
Text2_TFIDFStem<-dfm_tfidf(dfm(Text2_DFM, stem=TRUE))
Text3_TFIDFStem<-dfm_tfidf(dfm(Text3_DFM, stem=TRUE))

#TF-IDF No stop, No Punc, Words Stemmed
Text1_TFIDF<-dfm_tfidf(dfm(corpus[1], remove=stopwords('english'), remove_punct=TRUE, stem=TRUE))
Text2_TFIDF<-dfm_tfidf(dfm(corpus[2], remove=stopwords('english'), remove_punct=TRUE, stem=TRUE))
Text3_TFIDF<-dfm_tfidf(dfm(corpus[3], remove=stopwords('english'), remove_punct=TRUE, stem=TRUE))

```


### Task 6: Parts-of-Speech
For all texts, calculate the part-of-speech proportions and find out which POS tag is used most often by you in each text.

```{r}
#Load Library
require(qdap)

#Run Part-Of-Speech tagging
Text1_Tagged<-pos(corpus[1])
Text2_Tagged<-pos(corpus[2])
Text3_Tagged<-pos(corpus[3])

#Get POS proportions
Text1_Tagged$POSprop
Text2_Tagged$POSprop
Text3_Tagged$POSprop
```

